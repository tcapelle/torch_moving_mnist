{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1642a03b-b6d6-469c-a361-b20ad7e59921",
   "metadata": {},
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b74a08d-e48b-4f5b-a674-377cdc68694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "\n",
    "from unet import UNet, EMA\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74e44e6b-2619-4c70-8079-4667a5878b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(    \n",
    "    run_name = \"DDPM\",\n",
    "    epochs = 1,\n",
    "    noise_steps=1000,\n",
    "    seed = 42,\n",
    "    batch_size = 8,\n",
    "    img_size = 64,\n",
    "    num_frames = 4,\n",
    "    device = \"cuda\",\n",
    "    use_wandb = True,\n",
    "    do_validation = False,\n",
    "    fp16 = True,\n",
    "    num_workers=8,\n",
    "    train_steps=1000,\n",
    "    lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a163931-6e16-467a-9fd0-ab83b28bb112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_moving_mnist.data import MovingMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71bf9246-ee7b-4e0f-a2f8-21084bdbb832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n"
     ]
    }
   ],
   "source": [
    "affine_params = dict(\n",
    "    angle=(-20, 20), # rotation in degrees (min and max values)\n",
    "    translate=((-30, 30), (-30, 30)), # translation in pixels x and y\n",
    "    scale=(.8, 1.3), # scaling in percentage (1.0 = no scaling)\n",
    "    shear=(-20, 20), # deformation on the z-plane\n",
    ")\n",
    "\n",
    "ds = MovingMNIST(affine_params=affine_params, num_digits=[1,2], num_frames=config.num_frames*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ff5a596-f9ee-4d3d-b029-39b442c095ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bcd26cf-977c-452f-8f42-b6e984cbfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(c_in=8, c_out=8, time_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e5a9861-2283-4750-bbf5-b8d43b134cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(torch.rand(1,8,64,64), torch.tensor([1]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2aa73e41-071c-4803-8fb3-de367798ceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 1, 64, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ds.get_batch()\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ea1286a-5a6a-401a-81ae-1ef0cc74126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(bs=config.batch_size, frames=config.num_frames, dim=1, device=config.device):\n",
    "    b = ds.get_batch(bs).squeeze().to(device)\n",
    "    return b.split(frames, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "818a2e9f-cd40-4613-abf6-242a21872526",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc5bb20e-b3a3-4d45-84e0-98da8db5f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4, 64, 64]), torch.Size([8, 4, 64, 64]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc00da3c-1cb1-4502-a911-65959c8127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDiffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, c_in=8, c_out=4, use_wandb=False, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.model = UNet(c_in, c_out).to(device)\n",
    "        self.ema_model = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "        self.device = device\n",
    "        self.c_in = c_in\n",
    "        self.use_wandb=use_wandb\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        \"Add noise to images at instant t\"\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "    \n",
    "    \n",
    "    def noise_future(self, past_frames, future_frames, zero_past=False):\n",
    "        noise = torch.randn_like(future_frames)\n",
    "        if zero_past:\n",
    "            return torch.cat([torch.zeros_like(past_frames), noise], dim=1)\n",
    "        else:\n",
    "            return torch.cat([past_frames, noise], dim=1)\n",
    "    \n",
    "    def int_image(self, x):\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        return (x * 255).type(torch.uint8)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample(self, use_ema):\n",
    "        logging.info(f\"Sampling new images....\")\n",
    "        model = self.ema_model if use_ema else self.model\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            past_frames, future_frames = get_batch()\n",
    "            n = len(past_frames)\n",
    "            x = torch.randn_like(future_frames)\n",
    "            for i in progress_bar(reversed(range(1, self.noise_steps)), total=self.noise_steps-1, leave=False):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                all_frames = torch.cat([past_frames, x], dim=1)\n",
    "                predicted_noise = model(all_frames, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        x = self.int_image(x)\n",
    "        return torch.cat([self.int_image(past_frames), x], dim=1)\n",
    "\n",
    "    def train_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.ema.step_ema(self.ema_model, self.model)\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def train(self, train_steps, use_wandb=False):\n",
    "        self.model.train()\n",
    "        pbar = progress_bar(range(train_steps))\n",
    "        for i in pbar:\n",
    "            if i % 10_000 == 0:\n",
    "                self.log_images()\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                past_frames, future_frames = get_batch()\n",
    "                t = self.sample_timesteps(past_frames.shape[0]).to(self.device)  # batch size\n",
    "                x_t, noise = self.noise_images(future_frames, t)\n",
    "                all_frames = torch.cat([past_frames, x_t], dim=1)\n",
    "                predicted_noise = self.model(all_frames, t)\n",
    "                loss = self.mse(noise, predicted_noise)\n",
    "                self.train_step(loss)\n",
    "                if self.use_wandb:\n",
    "                    wandb.log({\"train_mse\": loss.item(),\n",
    "                               \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
    "            pbar.comment = f\"MSE={loss.item():2.3f}\"\n",
    "\n",
    "    def log_images(self):\n",
    "        \"Log images to wandb and save them to disk\"\n",
    "        sampled_images = self.sample(use_ema=False)\n",
    "        ema_sampled_images = self.sample(use_ema=True)\n",
    "        def to_image(img):\n",
    "            return wandb.Image(img.reshape(8*64, 64).transpose(1,0).cpu().numpy())\n",
    "        wandb.log({\"sampled_images\":     [to_image(img) for img in sampled_images]})\n",
    "        wandb.log({\"ema_sampled_images\": [to_image(img) for img in ema_sampled_images]})\n",
    "\n",
    "    def load(self, model_cpkt_path, model_ckpt=\"ckpt.pt\", ema_model_ckpt=\"ema_ckpt.pt\"):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(model_cpkt_path, model_ckpt)))\n",
    "        self.ema_model.load_state_dict(torch.load(os.path.join(model_cpkt_path, ema_model_ckpt)))\n",
    "\n",
    "    def save_model(self, run_name, use_wandb=False, epoch=-1):\n",
    "        \"Save model locally and on wandb\"\n",
    "        torch.save(self.model.state_dict(), os.path.join(\"models\", run_name, f\"ckpt.pt\"))\n",
    "        torch.save(self.ema_model.state_dict(), os.path.join(\"models\", run_name, f\"ema_ckpt.pt\"))\n",
    "        torch.save(self.optimizer.state_dict(), os.path.join(\"models\", run_name, f\"optim.pt\"))\n",
    "        if use_wandb:\n",
    "            at = wandb.Artifact(\"model\", type=\"model\", description=\"Model weights for DDPM conditional\", metadata={\"epoch\": epoch})\n",
    "            at.add_dir(os.path.join(\"models\", run_name))\n",
    "            wandb.log_artifact(at)\n",
    "\n",
    "    def prepare(self, args=None):\n",
    "        self.train_steps = args.train_steps\n",
    "        device = args.device\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=0.001)\n",
    "        # self.scheduler = optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.lr, \n",
    "        #                                          steps_per_epoch=args.train_steps, epochs=args.epochs)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=args.train_steps)\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.ema = EMA(0.995)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def fit(self, args):\n",
    "        self.train(args.train_steps, use_wandb=args.use_wandb)\n",
    "            \n",
    "            \n",
    "        # self.save_model(run_name=args.run_name, use_wandb=args.use_wandb, epoch=epoch)\n",
    "\n",
    "        # save model\n",
    "        # self.save_model(run_name=args.run_name, use_wandb=args.use_wandb, epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb2716-66d7-4993-aa58-9be75dd6e2bf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "203db3b6-cb2a-46c2-b527-70262b8100b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.noise_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b43a421-a264-4f77-ba68-99ee70d64186",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_steps = 250_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a2ec13-951b-4ab8-997b-5210beba7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = FrameDiffusion(noise_steps=config.noise_steps, device=config.device, use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972e8ce-4691-40fd-83db-7b1310e487cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/wandb/torch_moving_mnist/nbs/wandb/run-20230112_214138-2iku6n8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/ddpm_mmnist/runs/2iku6n8e\" target=\"_blank\">jolly-vortex-19</a></strong> to <a href=\"https://wandb.ai/capecape/ddpm_mmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='250000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/250000 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:41:44 - INFO: Sampling new images....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='195' class='' max='999' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      19.52% [195/999 00:07&lt;00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"ddpm_mmnist\", group=\"train\", config=config) if config.use_wandb else nullcontext():\n",
    "    diffuser.prepare(config)\n",
    "    diffuser.fit(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae309ae-4604-47ce-bf13-bd56bafb996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo poweroff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
