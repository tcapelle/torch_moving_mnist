{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1642a03b-b6d6-469c-a361-b20ad7e59921",
   "metadata": {},
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74a08d-e48b-4f5b-a674-377cdc68694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "\n",
    "from unet import UNet, EMA\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e44e6b-2619-4c70-8079-4667a5878b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(    \n",
    "    run_name = \"DDPM\",\n",
    "    epochs = 1,\n",
    "    noise_steps=1000,\n",
    "    seed = 42,\n",
    "    batch_size = 2,\n",
    "    img_size = 64,\n",
    "    num_frames = 4,\n",
    "    device = \"cpu\",\n",
    "    use_wandb = True,\n",
    "    do_validation = False,\n",
    "    fp16 = True,\n",
    "    log_every_epoch = 10,\n",
    "    num_workers=8,\n",
    "    train_steps=1000,\n",
    "    lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a163931-6e16-467a-9fd0-ab83b28bb112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_moving_mnist.data import MovingMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf9246-ee7b-4e0f-a2f8-21084bdbb832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n"
     ]
    }
   ],
   "source": [
    "affine_params = dict(\n",
    "    angle=(-20, 20), # rotation in degrees (min and max values)\n",
    "    translate=((-30, 30), (-30, 30)), # translation in pixels x and y\n",
    "    scale=(.8, 1.3), # scaling in percentage (1.0 = no scaling)\n",
    "    shear=(-20, 20), # deformation on the z-plane\n",
    ")\n",
    "\n",
    "ds = MovingMNIST(affine_params=affine_params, num_digits=[1,2], num_frames=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5a596-f9ee-4d3d-b029-39b442c095ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd26cf-977c-452f-8f42-b6e984cbfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(c_in=8, c_out=8, time_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a9861-2283-4750-bbf5-b8d43b134cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(torch.rand(1,8,64,64), torch.tensor([1]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa73e41-071c-4803-8fb3-de367798ceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 1, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ds.get_batch()\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1286a-5a6a-401a-81ae-1ef0cc74126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(bs=config.batch_size, frames=config.num_frames, dim=1, device=config.device):\n",
    "    b = ds.get_batch(bs).squeeze().to(device)\n",
    "    return b.split(frames, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a2e9f-cd40-4613-abf6-242a21872526",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bb20e-b3a3-4d45-84e0-98da8db5f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 64, 64]), torch.Size([2, 4, 64, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00da3c-1cb1-4502-a911-65959c8127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDiffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, c_in=8, c_out=8, use_wandb=False, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.model = UNet(c_in, c_out).to(device)\n",
    "        self.ema_model = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "        self.device = device\n",
    "        self.c_in = c_in\n",
    "        self.use_wandb=use_wandb\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        \"Add noise to images at instant t\"\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "    \n",
    "    \n",
    "    def noise_future(self, past_frames, future_frames, zero_past=False):\n",
    "        noise = torch.randn_like(future_frames)\n",
    "        if zero_past:\n",
    "            return torch.cat([torch.zeros_like(past_frames), noise], dim=1)\n",
    "        else:\n",
    "            return torch.cat([past_frames, noise], dim=1)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def sample(self, use_ema):\n",
    "        logging.info(f\"Sampling new images....\")\n",
    "        model = self.ema_model if use_ema else self.model\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            past_frames, future_frames = get_batch()\n",
    "            n = len(past_frames)\n",
    "            x = self.noise_future(past_frames, future_frames)\n",
    "            for i in progress_bar(reversed(range(1, self.noise_steps)), total=self.noise_steps-1, leave=False):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = self.noise_future(past_frames, future_frames, zero_past=True)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return torch.cat([past_frames, x[:,4:,...]], dim=1)\n",
    "\n",
    "    def train_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.ema.step_ema(self.ema_model, self.model)\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def train(self, train_steps, use_wandb=False):\n",
    "        avg_loss = 0.\n",
    "        self.model.train()\n",
    "        pbar = progress_bar(range(train_steps))\n",
    "        for i in pbar:\n",
    "            if i % 10 == 0:\n",
    "                self.log_images()\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                past_frames, future_frames = get_batch()\n",
    "                t = self.sample_timesteps(past_frames.shape[0]).to(self.device)\n",
    "                x_t, noise = self.noise_images(future_frames, t)\n",
    "                all_frames = torch.cat([past_frames, x_t], dim=1)\n",
    "                predicted_noise = self.model(all_frames, t)\n",
    "                loss = self.mse(noise, predicted_noise[:,4:,...])\n",
    "                avg_loss += loss\n",
    "                self.train_step(loss)\n",
    "                if self.use_wandb:\n",
    "                    wandb.log({\"train_mse\": loss.item(),\n",
    "                               \"learning_rate\": self.scheduler.get_last_lr()[0]})\n",
    "            pbar.comment = f\"MSE={loss.item():2.3f}\"        \n",
    "        return avg_loss.mean().item()\n",
    "\n",
    "    def log_images(self):\n",
    "        \"Log images to wandb and save them to disk\"\n",
    "        sampled_images = self.sample(use_ema=False)\n",
    "        ema_sampled_images = self.sample(use_ema=True)\n",
    "        def to_image(img):\n",
    "            return wandb.Image(img.reshape(8*64, 64).transpose(1,0).cpu().numpy())\n",
    "        wandb.log({\"sampled_images\":     [to_image(img) for img in sampled_images]})\n",
    "        wandb.log({\"ema_sampled_images\": [to_image(img) for img in ema_sampled_images]})\n",
    "\n",
    "    def load(self, model_cpkt_path, model_ckpt=\"ckpt.pt\", ema_model_ckpt=\"ema_ckpt.pt\"):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(model_cpkt_path, model_ckpt)))\n",
    "        self.ema_model.load_state_dict(torch.load(os.path.join(model_cpkt_path, ema_model_ckpt)))\n",
    "\n",
    "    def save_model(self, run_name, use_wandb=False, epoch=-1):\n",
    "        \"Save model locally and on wandb\"\n",
    "        torch.save(self.model.state_dict(), os.path.join(\"models\", run_name, f\"ckpt.pt\"))\n",
    "        torch.save(self.ema_model.state_dict(), os.path.join(\"models\", run_name, f\"ema_ckpt.pt\"))\n",
    "        torch.save(self.optimizer.state_dict(), os.path.join(\"models\", run_name, f\"optim.pt\"))\n",
    "        if use_wandb:\n",
    "            at = wandb.Artifact(\"model\", type=\"model\", description=\"Model weights for DDPM conditional\", metadata={\"epoch\": epoch})\n",
    "            at.add_dir(os.path.join(\"models\", run_name))\n",
    "            wandb.log_artifact(at)\n",
    "\n",
    "    def prepare(self, args=None):\n",
    "        self.train_steps = args.train_steps\n",
    "        device = args.device\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=0.001)\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.lr, \n",
    "                                                 steps_per_epoch=args.train_steps, epochs=args.epochs)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ema = EMA(0.995)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def fit(self, args):\n",
    "        self.train(args.train_steps, use_wandb=args.use_wandb)\n",
    "            \n",
    "            \n",
    "        # self.save_model(run_name=args.run_name, use_wandb=args.use_wandb, epoch=epoch)\n",
    "\n",
    "        # save model\n",
    "        # self.save_model(run_name=args.run_name, use_wandb=args.use_wandb, epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb2716-66d7-4993-aa58-9be75dd6e2bf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43a421-a264-4f77-ba68-99ee70d64186",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2ec13-951b-4ab8-997b-5210beba7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = FrameDiffusion(noise_steps=10, device=config.device, use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef8fa8-27b0-49a4-9114-4cbe337830f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:41:32 - INFO: Sampling new images....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = diffuser.sample(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972e8ce-4691-40fd-83db-7b1310e487cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9c68d68f1b4db9b598e8183279a57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016757584716348597, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/wandb/torch_moving_mnist/nbs/wandb/run-20230112_214134-103hrqld</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/ddpm_mmnist/runs/103hrqld\" target=\"_blank\">lemon-bee-7</a></strong> to <a href=\"https://wandb.ai/capecape/ddpm_mmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:41:47 - INFO: Sampling new images....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:41:50 - INFO: Sampling new images....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a34b52b02a343d08b6d8268eee5b8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.055 MB of 0.055 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▅██▇▅▄▂▁▁▁</td></tr><tr><td>train_mse</td><td>▆▃█▅▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>train_mse</td><td>1.0331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lemon-bee-7</strong>: <a href=\"https://wandb.ai/capecape/ddpm_mmnist/runs/103hrqld\" target=\"_blank\">https://wandb.ai/capecape/ddpm_mmnist/runs/103hrqld</a><br/>Synced 6 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230112_214134-103hrqld/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"ddpm_mmnist\", group=\"train\", config=config) if config.use_wandb else nullcontext():\n",
    "    diffuser.prepare(config)\n",
    "    diffuser.fit(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
