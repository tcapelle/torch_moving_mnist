# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% auto 0
__all__ = ['mnist_stats', 'affine_params', 'padding', 'linear_schedule', 'random_linear_schedule', 'Trajectory', 'MovingMNIST']

# %% ../nbs/01_data.ipynb 2
from types import SimpleNamespace
from torchvision.datasets import MNIST

# %% ../nbs/01_data.ipynb 10
import random

import torch
import numpy as np
import torch.nn.functional as F
import torchvision.transforms as T
import torchvision.transforms.functional as TF

mnist_stats    = ([0.131], [0.308])

# %% ../nbs/01_data.ipynb 17
def padding(img_size=128, mnist_size=28): return (img_size - mnist_size) // 2

# %% ../nbs/01_data.ipynb 21
def linear_schedule(a, b, n=5):
    "equivalent to np.linspace"
    return [i*(b-a)/(n-1) + a for i in range(n)]

# %% ../nbs/01_data.ipynb 24
affine_params = SimpleNamespace(
    angle=(-20, 20),
    translate=((-30, 30), (-30, 30)),
    scale=(.8, 1.3),
    shear=(-20, 20),
)

# %% ../nbs/01_data.ipynb 25
def random_linear_schedule(a, b, n=5):
    x = random.uniform(a,b)
    y = random.uniform(a,b)
    return linear_schedule(x, y, n=n)

# %% ../nbs/01_data.ipynb 28
class Trajectory:
    def __init__(self, angle, translate, scale, shear, n=5):
        self.angle_schedule = random_linear_schedule(*angle, n=n)
        self.scale_schedule = random_linear_schedule(*scale, n=n)
        translate_x = random_linear_schedule(*translate[0], n=n)
        translate_y = random_linear_schedule(*translate[1], n=n)
        self.translate_schedule = list(zip(translate_x, translate_y))
        self.shear_schedule = random_linear_schedule(*shear, n=n)
        
    def points(self):
        return list(zip(self.angle_schedule , self.translate_schedule, self.scale_schedule, self.shear_schedule))
            
    def apply(self, img):
        return [TF.affine(img, *param) for param in self.points()]

# %% ../nbs/01_data.ipynb 31
import math
import random

class MovingMNIST:
    def __init__(self, path=".",  # path to store the MNIST dataset
                 affine_params: dict=affine_params, # affine transform parameters, refer to torchvision.transforms.functional.affine
                 num_digits: list[int]=[3], # how many digits to move, random choice between the value provided
                 num_frames: int=4, # how many frames to create
                 img_size=64, # the canvas size, the actual digits are always 28x28
                 concat=True, # if we concat the final results (frames, 1, 28, 28) or a list of frames.
                 normalize=True # scale images in [0,1] and normalize them with MNIST stats. Applied at batch level. Have to take care of the canvas size that messes up the stats!
                ):
        self.mnist = MNIST(path, download=True).data
        self.affine_params = affine_params
        self.num_digits = num_digits
        self.num_frames = num_frames
        self.img_size = img_size
        self.pad = padding(img_size)
        self.concat = concat
        
        # some computation to ensure normalizing correctly-ish
        batch_tfms = [T.ConvertImageDtype(torch.float32)]
        if normalize:
            ratio = (28/img_size)**2*max(num_digits)
            mean, std = mnist_stats
            scaled_mnist_stats = ([mean[0]*ratio], [std[0]*ratio])
            print(f"New computed stats for MovingMNIST: {scaled_mnist_stats}")
            batch_tfms += [T.Normalize(*scaled_mnist_stats)] if normalize else []
        self.batch_tfms = T.Compose(batch_tfms)  
    
    def random_digit(self):
        img = self.mnist[[random.randrange(0, len(self.mnist))]]
        return TF.pad(img, padding=self.pad) 
    
    def random_trajectory(self):
        return Trajectory(**self.affine_params, n=self.num_frames)
    
    def _one_moving_digit(self):
        digit = self.random_digit()
        traj = self.random_trajectory()
        return torch.stack(traj.apply(digit))
    
    def __getitem__(self, i):
        moving_digits = [self._one_moving_digit() for _ in range(random.choice(self.num_digits))]
        moving_digits = torch.stack(moving_digits)
        combined_digits = moving_digits.max(dim=0)[0]
        return combined_digits if self.concat else [t.squeeze(dim=0) for t in combined_digits.split(1)]
    
    def get_batch(self, bs=32):
        "Grab a batch of data"
        batch = torch.stack([self[0] for _ in range(bs)])
        return self.batch_tfms(batch) if self.batch_tfms is not None else batch
